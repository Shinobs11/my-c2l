{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, pickle, torch, logging, typing, numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertForMaskedLM, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "import transformers as T\n",
    "from logging.handlers import RotatingFileHandler\n",
    "from src.utils.pickleUtils import pdump, pload, pjoin\n",
    "\n",
    "from src.proecssing import correct_count\n",
    "from transformers.models.bert.modeling_bert import SequenceClassifierOutput\n",
    "from src.classes.datasets import DataItem\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_formatter = logging.Formatter('%(asctime)s %(levelname)s %(funcName)s(%(lineno)d) %(message)s')\n",
    "def setup_logger(name, log_file, level=logging.INFO):\n",
    "  handler = logging.FileHandler(log_file, mode='w')\n",
    "  handler.setFormatter(log_formatter)\n",
    "  logger = logging.getLogger(name)\n",
    "  logger.setLevel(level)\n",
    "  logger.addHandler(handler)\n",
    "  return logger\n",
    "\n",
    "error_logger = setup_logger(\"error_log\", \"error_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"imdb\"\n",
    "DATASET_NAME = \"original_augmented_1x_aclImdb\"\n",
    "DATASET_PATH = f\"./datasets/{DATASET_NAME}/base\"\n",
    "OUTPUT_PATH = f\"checkpoints/{DATASET_NAME}/model\"\n",
    "TRIPLETS_PATH = f\"./datasets/{DATASET_NAME}/augmented_triplets\"\n",
    "TOPK_NUM = 4\n",
    "\n",
    "\n",
    "import json, psutil\n",
    "# env = {}\n",
    "# with open(\"./env.json\", mode=\"r\") as f:\n",
    "#   env = json.load(f)\n",
    "\n",
    "\n",
    "# memAvailable = psutil.virtual_memory().available\n",
    "# estimatedMemConsumed = os.path.getsize(os.path.join(DATASET_PATH, \"train_set.pickle.blosc\")) * 3\n",
    "# USE_PINNED_MEMORY = True if (env['USE_PINNED_MEMORY'] & (memAvailable > estimatedMemConsumed)) == 1 else False\n",
    "\n",
    "\n",
    "sampling_ratio = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer: T.BertTokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "# train_set = pload(os.path.join(DATASET_PATH, \"train_set\"))\n",
    "f = open(os.path.join(DATASET_PATH, \"train_set.json\"))\n",
    "data = json.load(f)\n",
    "SPLIT_SAMPLES = 1000000\n",
    "NOTEBOOK_INDEX = 0\n",
    "train_texts = [d['anchor_text'] for d in data][SPLIT_SAMPLES * (NOTEBOOK_INDEX):SPLIT_SAMPLES * (NOTEBOOK_INDEX + 1)]\n",
    "train_labels = [d['label'] for d in data][SPLIT_SAMPLES * (NOTEBOOK_INDEX):SPLIT_SAMPLES * (NOTEBOOK_INDEX + 1)]\n",
    "train_texts = train_texts[0:100]\n",
    "train_labels = train_labels[0:100]\n",
    "# train_texts:list[str] = train_set['review'].tolist()[0:100]\n",
    "# train_labels:list = train_set['sentiment'].tolist()[0:100]\n",
    "train_encodings = tokenizer(train_texts, padding=True, truncation=True)\n",
    "# pdump(train_encodings, os.path.join(DATASET_PATH, \"train_encodings\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# valid_set = pload(os.path.join(DATASET_PATH, \"valid_set\"))\n",
    "# valid_texts:list[str] = valid_set['review'].tolist()\n",
    "# valid_labels: list = valid_set['sentiment'].tolist()\n",
    "# valid_encodings = tokenizer(valid_texts, padding=True, truncation=True)\n",
    "# pdump(valid_encodings, os.path.join(DATASET_PATH, \"valid_encodings\"))\n",
    "# print(train_encodings.keys())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from  src.classes.datasets import IMDBDataset\n",
    "class IMDBDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "train_dataset = IMDBDataset(labels=train_labels, encodings=train_encodings)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "  train_dataset,\n",
    "  batch_size=1,\n",
    "  shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model:BertForSequenceClassification = BertForSequenceClassification.from_pretrained(os.path.join(OUTPUT_PATH, 'best_epoch')) #type:ignore\n",
    "model.to(device)\n",
    "def get_gradient_norms(batch):\n",
    "  input_ids:Tensor = batch['input_ids'].to(device)\n",
    "  attention_mask:Tensor = batch['attention_mask'].to(device)\n",
    "  labels:Tensor = batch['labels'].to(device)\n",
    "\n",
    "  _, labels = torch.max(labels, dim=1)\n",
    "\n",
    "\n",
    "  outputs:SequenceClassifierOutput | tuple[Tensor] = model.forward(input_ids=input_ids, attention_mask=attention_mask, labels=labels, return_dict=True)\n",
    "  assert isinstance(outputs, SequenceClassifierOutput)\n",
    "  \n",
    "  loss = outputs['loss']\n",
    "  loss.backward(retain_graph=True)\n",
    "  torch.cuda.empty_cache()\n",
    "  importances = torch.tensor([]).to(device)\n",
    "  for pos_index, token_index in zip(range(1, len(input_ids[0])), input_ids[0][1:]):\n",
    "    if token_index == tokenizer.sep_token_id:\n",
    "      break\n",
    "    importance = torch.norm(model.bert.embeddings.position_embeddings.weight.grad[pos_index], 2).float().detach() #type:ignore\n",
    "    importances = torch.cat((importances, importance.unsqueeze(0)), dim=-1)\n",
    "  \n",
    "  model.bert.embeddings.position_embeddings.weight.grad = None #! why???\n",
    "\n",
    "  return importances\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_importances(data_loader:DataLoader, importance_function:typing.Callable) -> list[Tensor]:\n",
    "  all_importances: list = []\n",
    "  for batch in tqdm(data_loader):\n",
    "    importances = importance_function(batch)\n",
    "    all_importances.append(importances)\n",
    "  return all_importances\n",
    "\n",
    "def compute_average_importance(data_loader:DataLoader, all_importances) -> list[Tensor]:\n",
    "  all_averaged_importances:list = []\n",
    "  importance_dict = dict()\n",
    "  importance_dict_counter = dict()\n",
    "\n",
    "  for importances, batch in tqdm(zip(all_importances, data_loader)):\n",
    "    tokens = [x for x in batch['input_ids'][0][1:] if x not in [tokenizer.sep_token_id, tokenizer.pad_token_id]]\n",
    "\n",
    "    for token_importance, token in zip(importances, tokens):\n",
    "      if not token in importance_dict.keys():\n",
    "        importance_dict[token.item()] = 0\n",
    "        importance_dict_counter[token.item()] = 0\n",
    "      importance_dict[token.item()] += token_importance\n",
    "      importance_dict_counter[token.item()] += 1\n",
    "    \n",
    "  for importances, batch in tqdm(zip(all_importances, data_loader)):\n",
    "    tokens = [x for x in batch['input_ids'][0][1:] if x not in [tokenizer.sep_token_id, tokenizer.pad_token_id]]\n",
    "    averaged_importances = torch.Tensor([importance_dict[x.item()]/importance_dict_counter[x.item()] for x in tokens])\n",
    "    all_averaged_importances.append(averaged_importances)\n",
    "  return all_averaged_importances\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 37.62it/s]\n",
      "100it [00:00, 847.92it/s]\n",
      "100it [00:00, 688.71it/s]\n"
     ]
    }
   ],
   "source": [
    "importancePath = os.path.join(DATASET_PATH, \"train_set_importance\")\n",
    "importance: list[Tensor] = []\n",
    "# if(os.path.exists(pjoin(importancePath))):\n",
    "#   importance = pload(importancePath)\n",
    "# else:\n",
    "#   importance = compute_importances(train_loader, get_gradient_norms)\n",
    "#   pdump(importance, importancePath)\n",
    "importance = compute_importances(train_loader, get_gradient_norms)\n",
    "\n",
    "averageImportancePath = os.path.join(DATASET_PATH, \"train_set_average_importance\")\n",
    "averageImportance: list[Tensor] = []\n",
    "# if(os.path.exists(pjoin(averageImportancePath))):\n",
    "#   averageImportance = pload(averageImportancePath)\n",
    "\n",
    "# else:\n",
    "#   averageImportance = compute_average_importance(train_loader, importance)\n",
    "#   pdump(averageImportance, averageImportancePath)\n",
    "averageImportance = compute_average_importance(train_loader, importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mlm_model: BertForMaskedLM = BertForMaskedLM.from_pretrained('bert-base-uncased') #type: ignore\n",
    "mlm_model: BertForMaskedLM = mlm_model.to(device) #type: ignore\n",
    "mlm_model.eval()\n",
    "\n",
    "def mask_data(data_loader:DataLoader, all_importances: list[Tensor], sampling_ratio, augment_ratio):\n",
    "  triplets = []\n",
    "  error_count = 0\n",
    "  no_flip_count = 0\n",
    "  no_flip_index = []\n",
    "\n",
    "  for importances, batch in tqdm(zip(all_importances, data_loader)):\n",
    "    label = []\n",
    "    tokens = torch.tensor([x for x in batch['input_ids'][0][1:] if x not in [tokenizer.sep_token_id, tokenizer.pad_token_id]]) # could this be done better?\n",
    "    assert tokens.size() == importances.size()\n",
    "    \n",
    "    orig_sample = tokenizer.decode(tokens)\n",
    "    causal_mask, err_flag, maximum_score = mask_causal_words(tokens.cpu().numpy(), batch, importances.cpu().numpy(), topk=sampling_ratio)\n",
    "    no_flip_index.append(err_flag)\n",
    "    if err_flag:\n",
    "      no_flip_count += 1\n",
    "    \n",
    "    if 1 not in causal_mask:\n",
    "      triplets.append((label, orig_sample, orig_sample, orig_sample, err_flag, maximum_score))\n",
    "      continue\n",
    "\n",
    "    for _ in range(augment_ratio):\n",
    "      causal_masked_tokens = []\n",
    "      noncausal_masked_tokens = []\n",
    "\n",
    "      if sampling_ratio is None:\n",
    "        causal_masked_tokens = [tokens[i] if causal_mask[i] == 0 else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "        noncausal_masked_tokens = [tokens[i] if causal_mask[i] == 1 else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "\n",
    "      elif type(sampling_ratio) == int:\n",
    "        causal_indices = np.where(np.array(causal_mask) == 1)[0]\n",
    "        noncausal_indices = np.where(np.array(causal_mask) == 0)[0]\n",
    "\n",
    "        causal_mask_indices = np.random.choice(causal_indices, sampling_ratio)\n",
    "\n",
    "        try:\n",
    "          noncausal_mask_indices = np.random.choice(noncausal_indices, max(1, min(sampling_ratio, len(noncausal_indices))))\n",
    "        except:\n",
    "          noncausal_mask_indices = np.random.choice(causal_indices, sampling_ratio)\n",
    "          error_count += 1\n",
    "\n",
    "        causal_masked_tokens = [tokens[i] if i not in causal_mask_indices else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "        noncausal_masked_tokens = [tokens[i] if i not in noncausal_mask_indices else tokenizer.mask_token_id for i in range(len(tokens))]\n",
    "      else:\n",
    "        pass\n",
    "\n",
    "      causal_masked_sample = tokenizer.decode(causal_masked_tokens)\n",
    "      noncausal_masked_sample = tokenizer.decode(noncausal_masked_tokens)\n",
    "\n",
    "      _, labels = torch.max(batch['labels'], dim=1)\n",
    "   \n",
    "      if labels[0] == 0: label = [0, 1]\n",
    "      elif labels[0] == 1: label = [1, 0]\n",
    "      triplets.append((label, orig_sample, causal_masked_sample, noncausal_masked_sample, err_flag, maximum_score))\n",
    "  print(f\"Error count: {error_count}\")\n",
    "  print(f\"No flip count: {no_flip_count}\")\n",
    "  return triplets, no_flip_index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def mask_causal_words(tokens:Tensor, batch:DataItem, importances: Tensor, topk=1):\n",
    "  dropout = torch.nn.Dropout(0.5)\n",
    "  causal_mask = [0 for _ in range(len(tokens))]\n",
    "  all_importance_indices = np.argsort(importances)[::-1]\n",
    "\n",
    "  err_flag = False\n",
    "  find_flag = False\n",
    "\n",
    "  input_ids:Tensor = batch['input_ids'].squeeze().repeat((TOPK_NUM,)).reshape(TOPK_NUM, -1).to(device)\n",
    "  attention_mask:Tensor = batch['attention_mask'].expand(TOPK_NUM, -1).to(device)\n",
    "  token_type_ids:Tensor = batch['token_type_ids'].expand(TOPK_NUM, -1).to(device) #! is token_type_ids actually used anywhere???\n",
    "\n",
    "  masked_input_ids = batch['input_ids'].squeeze().repeat((len(tokens),)).reshape(len(tokens), -1).to(device)\n",
    "  masked_attention_mask = batch['attention_mask'].expand(len(tokens), -1).to(device)\n",
    "  masked_token_type_ids = batch['token_type_ids'].expand(len(tokens), -1).to(device)\n",
    "\n",
    "  fake_labels = torch.ones((len(tokens), ))\n",
    "  \n",
    "  masked_train = IMDBDataset({\n",
    "    'input_ids': masked_input_ids,\n",
    "    'attention_mask': masked_attention_mask,\n",
    "    'token_type_ids': masked_token_type_ids,\n",
    "    'importance_indices': all_importance_indices\n",
    "  }, fake_labels)\n",
    "\n",
    "  masked_train_loader = DataLoader(masked_train, batch_size=4, shuffle=False)\n",
    "  logits = []\n",
    "  for masked_batch in masked_train_loader:\n",
    "    masked_input_ids = masked_batch['input_ids'].to(device) # 4 x 313\n",
    "    masked_attention_mask = masked_batch['attention_mask'].to(device) # 4 x 313\n",
    "    masked_token_type_ids = masked_batch['token_type_ids'].to(device) # 4 x 313\n",
    "    importance_indices = masked_batch['importance_indices'].to(device)# 4\n",
    "    masked_input_embeds: Tensor = mlm_model.bert.embeddings.word_embeddings(masked_input_ids) #4 x 313 x 768\n",
    "\n",
    "    #dropout some of the embeddings at random\n",
    "    for mi_i, topk_i in zip(range(masked_input_embeds.size(0)), importance_indices):\n",
    "      masked_input_embeds[mi_i][topk_i + 1] = dropout(masked_input_embeds[mi_i][topk_i + 1])\n",
    "    \n",
    "    #get predicted words from mlm model given the partially missing embeds\n",
    "    with torch.no_grad():\n",
    "      outputs = mlm_model(attention_mask = masked_attention_mask, token_type_ids = masked_token_type_ids, inputs_embeds = masked_input_embeds)\n",
    "      predictions = outputs[0] # 4 x 313 x 30522, just a casual 38 million numbers. shape(batch_size, sequence_length, config.vocab_size)\n",
    "    \n",
    "    #search through and find top k logits, in this case 4. \n",
    "    topk_logit_indices = torch.topk(predictions, TOPK_NUM, dim=-1)[1] # 4 sequences x 313 tokens x 4 \n",
    "\n",
    "    #for the top k most important tokens, get their respective k candidates\n",
    "    mask_candidates = [topk_logits[importance_index + 1] for importance_index, topk_logits in zip(importance_indices, topk_logit_indices)]\n",
    "\n",
    "    \n",
    "    for importance_index, mask_candidate in zip(importance_indices, mask_candidates):\n",
    "      if importances[importance_index] == 0:\n",
    "        continue\n",
    "      recon_input_ids = input_ids.clone()\n",
    "      for i, mc in enumerate(mask_candidate):\n",
    "        recon_input_ids[i][importance_index + 1] = mc\n",
    "      \n",
    "      with torch.no_grad():\n",
    "        recon_outputs = model(recon_input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        _, recon_prediction = torch.max(recon_outputs[0], dim=1)\n",
    "        # print(f\"{recon_outputs[0]}\")\n",
    "      if len(torch.unique(recon_prediction)) != 1:\n",
    "        \n",
    "        causal_mask[importance_index] = 1\n",
    "        find_flag = True\n",
    "        break\n",
    "\n",
    "\n",
    "    if find_flag:\n",
    "      break\n",
    "\n",
    "  if 1 not in causal_mask:\n",
    "    causal_mask[all_importance_indices[0]] = 1\n",
    "    err_flag = True\n",
    "    return causal_mask, err_flag, 0\n",
    "  \n",
    "  return causal_mask, err_flag, 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/tmp/ipykernel_125041/541250526.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/tmp/ipykernel_125041/541250526.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx])\n",
      "100it [02:12,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Cnt: 0\n",
      "No Flip Cnt: 83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sampling_ratio = 1\n",
    "augment_ratio = 1\n",
    "\n",
    "triplets_train, no_flip_idx_train = mask_data(train_loader, averageImportance, sampling_ratio=sampling_ratio, augment_ratio=augment_ratio)\n",
    "\n",
    "triplets_json:list[dict] = []\n",
    "triplets_pickle:dict[str,list] = {\n",
    "  \"labels\": [],\n",
    "  \"anchor_texts\": [],\n",
    "  \"positive_texts\": [],\n",
    "  \"negative_texts\": [],\n",
    "  \"triplet_sample_masks\": []\n",
    "}\n",
    "for x in triplets_train:\n",
    "  triplets_json.append(\n",
    "    {\n",
    "      \"label\": x[0],\n",
    "      \"anchor_text\": x[1],\n",
    "      \"positive_text\": x[3],\n",
    "      \"negative_text\": x[2],\n",
    "      \"triplet_sample_mask\": x[4]\n",
    "    }\n",
    "  )\n",
    "  triplets_pickle[\"labels\"].append(x[0])\n",
    "  triplets_pickle[\"anchor_texts\"].append(x[1])\n",
    "  triplets_pickle[\"positive_texts\"].append(x[3])\n",
    "  triplets_pickle[\"negative_texts\"].append(x[2])\n",
    "  triplets_pickle[\"triplet_sample_masks\"].append(x[4])\n",
    "\n",
    "\n",
    "if not os.path.exists(TRIPLETS_PATH):\n",
    "  os.mkdir(TRIPLETS_PATH)\n",
    "\n",
    "pdump(triplets_pickle, os.path.join(TRIPLETS_PATH, \"augmented_triplets\"))\n",
    "with open(os.path.join(TRIPLETS_PATH, \"augmented_triplets.json\"), mode='w') as f:\n",
    "  json.dump(triplets_json, f)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-c2l-GA-eQEwD-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1 (main, Jan 18 2023, 19:58:44) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "728870d21f0524f16882758c2f8a11b897e3fd4a14d69d9cab895b40aa759979"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
